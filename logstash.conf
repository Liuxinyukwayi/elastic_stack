input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["nginx-logs"]
    codec => "json"
    group_id => "logstash-group"
  }
}

filter {
  # 处理 access.log
  if [log][file][path] =~ "access.log" {
    grok {
      match => {
        "message" => "%{IP:client_ip} - %{USER:ident} \[%{HTTPDATE:timestamp}\] \"%{DATA:request}\" %{NUMBER:status:int} %{NUMBER:bytes:int} \"%{DATA:referrer}\" \"%{DATA:user_agent}\" \"%{DATA:forwarded}\""
      }
    }
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }
    if [request] =~ "^{" {
      json {
        source => "request"
        target => "request_json"
      }
    }
  }

  # 处理 error.log
  else if [log][file][path] =~ "error.log" {
    grok {
      match => { 
        "message" => "%{YEAR}/%{MONTHNUM}/%{MONTHDAY} %{TIME} \[%{LOGLEVEL:loglevel}\] %{NUMBER:pid}#%{NUMBER:thread}: \*(%{NUMBER:request_id}) %{GREEDYDATA:error_message}" 
      }
    }
    date {
      match => [ "message", "YYYY/MM/dd HH:mm:ss" ]
      target => "@timestamp"
    }
  }

  # 通用处理
  mutate {
    remove_tag => ["_grokparsefailure"]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "nginx-logs-%{+YYYY.MM.dd}"
  }
  stdout { codec => rubydebug }
}

